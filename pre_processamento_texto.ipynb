{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pontuacoes = string.punctuation + '…' + '‡' + '·' + '–' + 'º' + '§'\n",
    "stopwords_portugues = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords_portugues = stopwords_portugues + [\"é\", \"ser\", \"ter\", \"dá\", \"art\"]\n",
    "\n",
    "numerais_romanos_pattern = re.compile('(^(?=[mdclxvi])M*(c[md]|d?c{0,3})(x[cl]|l?x{0,3})(i[xv]|v?i{0,3})$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(filename):\n",
    "    with open(filename) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def dump_json(filename, json_object):\n",
    "    with open(filename, \"w\") as fp:\n",
    "        json.dump(json_object, fp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"normas_80assuntos\"\n",
    "normas = read_json(f\"{filename}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processa_texto(text):\n",
    "    # Lower\n",
    "    text = text.lower()\n",
    "    # Tokenização:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remoção de stopwords:\n",
    "    tokens = [t for t in tokens if t not in stopwords_portugues]\n",
    "    # Remoção de números:\n",
    "    tokens = [re.sub(r'\\d+', '', t) for t in tokens]\n",
    "    # Remoção de pontuações:\n",
    "    tokens = [re.sub('['+pontuacoes+']','', t) for t in tokens]\n",
    "    # Remoção de números romanos:\n",
    "    tokens = [numerais_romanos_pattern.sub('', t) for t in tokens]\n",
    "    # Remoção de vazios:\n",
    "    tokens = [t for t in tokens if t and len(t) > 1] \n",
    "    # Remoção de acentos:\n",
    "    tokens = [unidecode(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def pre_processa_textos(normas, debug=False):\n",
    "    for i, norma in enumerate(normas):\n",
    "        if debug and i % 500 == 0:\n",
    "            print(f'Processando norma {i}')\n",
    "        norma['TextoPreProcessado'] = pre_processa_texto(norma['Texto'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processando norma 0\n",
      "Processando norma 500\n",
      "Processando norma 1000\n",
      "Processando norma 1500\n",
      "Processando norma 2000\n",
      "Processando norma 2500\n",
      "Processando norma 3000\n",
      "Processando norma 3500\n",
      "Processando norma 4000\n",
      "Processando norma 4500\n",
      "Processando norma 5000\n",
      "Processando norma 5500\n",
      "Processando norma 6000\n",
      "Processando norma 6500\n",
      "Processando norma 7000\n",
      "Processando norma 7500\n",
      "Processando norma 8000\n",
      "Processando norma 8500\n",
      "Processando norma 9000\n",
      "Processando norma 9500\n"
     ]
    }
   ],
   "source": [
    "pre_processa_textos(normas, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantém apenas campos 'TextoPreProcessado', 'AssuntoGeral', 'Norma' \n",
    "normas = [{key: norma[key] for key in norma if key in ['TextoPreProcessado', 'AssuntoGeral', 'Norma']} for norma in normas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_json(f\"{filename}_processadas.json\", normas);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}